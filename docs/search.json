[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website was created as a project for the course “DS 002R: Foundations of Data Science, R” at Pomona College.\nIn the Tab “Data Visualization”, there are two visualizations created using R, with data sourced from the TidyTuesday data repository. The TidyTuesday Project is a collection of data frames, released every week from 2018 to present day. The data collected in these frames are endless, with data quantifying Olympic Medals to data describing Himalayan Climbers. The project is open to anyone and everyone, and it aims to provide opportunities for individuals to learn how to work with real-world data sets through data science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Felicia Akinde",
    "section": "",
    "text": "Felicia Akinde is a student (CO ’27) at Pomona College in Claremont, California. She is majoring in neuroscience, with a minor in data science. Originally from Chicago, Illinois, she enjoys playing her guitar, drawing, and hanging out with friends when she isn’t soaking up the sunny California rays. One of her favorite food combinations is coffee and cheese, which inspired the data used on this website. Click on “About” to learn more!"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Option 1: tidytuesdayR package\ninstall.packages(“tidytuesdayR”)\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 20)\ncoffee_survey &lt;- tuesdata$coffee_survey"
  },
  {
    "objectID": "coffee_survey.html",
    "href": "coffee_survey.html",
    "title": "Coffee Survey",
    "section": "",
    "text": "library(tidytuesdayR)\nlibrary(tidyverse)\n\n# imports data from tidytuesday\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 20)\n\n# renames data\ncoffee_survey &lt;- tuesdata$coffee_survey\n\n# removes NAs from variables age and favorite\ncleaned_coffee_survey &lt;- coffee_survey |&gt;\n  filter(!is.na(age) & !is.na(favorite))\n\n# creates a new data frame with age and favorite, counts the occurences of each favorite per age group\ndata_summary &lt;- cleaned_coffee_survey |&gt;\n  group_by(age, favorite) |&gt;\n  summarize(count = n(), .groups = 'drop')\n\n# creates proportion, which is the percentage of favorite in age \ncoffee_survey3 &lt;- data_summary |&gt;\n  group_by(age)  |&gt;\n  mutate(proportion = count / sum(count, na.rm = TRUE))\n\n# makes graph\nggplot(coffee_survey3, \n       aes(x = favorite, y = proportion, fill = favorite)) +\n  geom_bar(stat = \"identity\") + \n  facet_wrap(~ age) +\n    \n  scale_y_continuous(\n    labels = scales::percent) +\n  labs(\n    x = \"Favorite Way to Drink Coffee\",\n    y = \"Percentage\",\n    title = \"Favorite Ways to Drink Coffee Across Different Age Groups\",\n  )+\n  theme(\n    axis.text.x = element_blank(),\n  )"
  },
  {
    "objectID": "cheese.html",
    "href": "cheese.html",
    "title": "Cheese",
    "section": "",
    "text": "Data Source:\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# imports data from tidytuesday\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 23)\n\n# renames data\ncheeses &lt;- tuesdata$cheeses\n\n# removes \"mg/100g\" from end of calcium_content\ncheeses$calcium_content &lt;- str_sub(cheeses$calcium_content, end = -8)\n\n# turns calcium_content from a string into a number\ncheeses$calcium_content &lt;- as.numeric(cheeses$calcium_content)\n\n# creates a new data point if there is more than one animal in milk\ncheeses_separated &lt;- cheeses |&gt;\n  separate_rows(milk, sep = \", \") \n\n# takes NAs out of milk and calcium content\ncleaned_cheeses &lt;- cheeses_separated |&gt;\n  filter(!is.na(milk) & !is.na(calcium_content)) |&gt;\n  # creates ranges for calcium_content & assigns them appropriate labels\n  mutate(calcium_range = cut(calcium_content,\n                         breaks = c(0, 50, 100, 200, 500, 1000, 5000),\n                         labels = c(\"0-50\", \"50-100\", \"100-200\", \"200-500\", \"500-1000\", \"1000-5000\")\n                         ))\n\n# assigns each range to a color \ncolors &lt;- c(\"0-50\" = \"#CDC754\",  \n            \"50-100\" = \"#80CD54\",\n            \"100-200\" = \"#54CD8D\",\n            \"200-500\" = \"#54C2CD\",  \n            \"500-1000\" = \"#7588F4\",\n            \"1000-5000\" = \"#2D34A7\") \n\n# makes plot\nggplot(cleaned_cheeses, aes(x = milk, y = color, fill = calcium_range)) +\n  geom_tile() +\n  scale_fill_manual(values = colors) +\n  labs(\n    x = \"Type of Milk\",\n    y = \"Color\",\n    title = \"Calcium Content by Type of Milk and Cheese Color\",\n    fill = \"Calcium Content (mg/100g)\"\n  )"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Mini-Project 2",
    "section": "",
    "text": "library(tidyverse)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 31)\n\nsummer_movie_genres &lt;- tuesdata$summer_movie_genres\nsummer_movies &lt;- tuesdata$summer_movies\n\n\n#\nsummer_movies_digi &lt;- summer_movies |&gt;\n  filter(str_detect(simple_title, \"\\\\d\")) |&gt;\n  select(primary_title, year, genres, simple_title, average_rating)\n\nsummer_movies_of_digi &lt;- summer_movies_digi |&gt;\n  filter(str_detect(simple_title, \"summer (..) \\\\d{2,}\"))\n\nsummer_movies_of_digi\n\n# A tibble: 16 × 5\n   primary_title                         year genres simple_title average_rating\n   &lt;chr&gt;                                &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;\n 1 Summer of '42                         1971 Drama… summer of 42            7.2\n 2 The Elusive Summer of '68             1984 Comed… the elusive…            8.5\n 3 The Cold Summer of 1953               1988 Actio… the cold su…            7.8\n 4 Summer of '69                         1969 Roman… summer of 69            5.8\n 5 Memories of the Olympic Summer of 1…  1954 Docum… memories of…            6  \n 6 A Boy's Summer in 1945                2002 Drama… a boys summ…            7  \n 7 Summer of 85                          2020 Drama… summer of 85            6.9\n 8 Saint Laurent: Summer of '21          2020 Short  saint laure…            6.5\n 9 The Summer of 69                      2009 Comedy the summer …            7.9\n10 Summer of 1941                        2022 Actio… summer of 1…            6  \n11 Elvis: Summer of '56                  2011 Docum… elvis summe…            6.4\n12 Summer of '92                         2015 Biogr… summer of 92            6.8\n13 Summer of 84                          2018 Horro… summer of 84            6.7\n14 Summer of '44                         2017 Drama… summer of 44            5.7\n15 Summer of '67                         2018 Drama… summer of 67            4.9\n16 Shashin Koshien Summer in 0.5 Secon…  2017 Drama  shashin kos…            7.5\n\n\nThis is a table created from a data set that contains movies with the word “Summer” in them. This new table shows if they have the sequence “Summer” plus any two letters (most commonly of or in), plus a sequence of digits if those digits are more than double digits.\n\n# genres and average rating (plot)\n\nmovies_gen_rating &lt;- summer_movies |&gt;\n  mutate(genres = str_split(genres, \",\")) |&gt;\n  mutate(perc_over_5 = str_count(average_rating, \"[5-9]\\\\.\\\\d+\")) |&gt;\n  unnest(genres) |&gt;\n  filter(!is.na(genres)) |&gt;\n  group_by(genres) |&gt;\n  summarize(perc_over_5 = sum(perc_over_5)) |&gt;\n  select(genres, perc_over_5)\n\nggplot(movies_gen_rating, \n       aes(x = genres, y = perc_over_5)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Summer Movie Genre\",\n    y = \"Amount of Ratings over 5.0\",\n    title = \"Amount of over 5.0 Ratings per Summer Movie Genre\",\n  )\n\n\n\n\n\n\n\n\nThis is a graph relating genres of movies with “Summer” in the title, and the amount of 5.0+ ratings movies with that genre received. This graph shows that Drama, Comedy, and Romance all got the most ratings over 5.0. Film-Noir, Reality-TV, Talk-Show, and Western got the least ratings over 5.0."
  },
  {
    "objectID": "deck_of_cards.html",
    "href": "deck_of_cards.html",
    "title": "Deck of Cards",
    "section": "",
    "text": "library(tidyverse)\n\nWith this analysis, I set out to calculate the probability of getting exactly one pair in a hand of cards of varying sizes (2-6). I planned to create a generalizable function that could work with any of these amounts. Finally, I planned to visualize these probabilities using a line graph.\n\n# assigns every literal number of card to a rank in order to be able to pull cards out of the \"deck\"\n\nAce &lt;- c(1, 14, 27, 40)\ntwo &lt;- c(2, 15, 28, 41)\nthree &lt;- c(3, 16, 29, 42)\nfour &lt;- c(4, 17, 30, 43)\nfive &lt;- c(5, 18, 31, 44)\nsix &lt;- c(6, 19, 32, 45)\nseven &lt;- c(7, 20, 33, 46)\neight &lt;- c(8, 21, 34, 47)\nnine &lt;- c(9, 22, 35, 48)\nten &lt;-c(10, 23, 36, 49)\njacks &lt;- c(11, 24, 37, 50)\nqueens &lt;- c(12, 25, 38, 51)\nkings &lt;- c(13, 26, 39, 52)\n\n\n\ndeck_of_cards &lt;- data.frame(Ace = Ace, Two = two, Three = three, \n                            Four = four, Five = five, Six = six, Seven = seven, \n                            Eight = eight, Nine = nine, Ten = ten, Jack = jacks, \n                            Queen = queens, King = kings\n                            )\n\n# Diamonds: 1 = Diamond Ace, 2-10 Diamond, 11 = Diamond Jack, 12 = Diamond Queen, 13 = Diamond King\n# Hearts: 14 = Heart Ace, 15-23(2-10 Heart), 24 = Heart Jack, 25 = Heart Queen, 26 = Heart King\n# Spades: 27 = Spade Ace, 28-36(2-10 Spade), 37 = Spade Jack, 38 = Spade Queen, 39 = Spade King\n# Clubs: 40 = Club Ace, 41-49(2-10 Club), 50 = Club Jack, 51 = Club Queen, 52 = Club King\n\n\n  \nhand_generator &lt;- function(num_of_cards) {\n   \n  num_cards_left &lt;- num_of_cards - 2\n  \n  simplified_deck &lt;- unlist(deck_of_cards)\n  hand &lt;- sample(simplified_deck, num_of_cards, replace = FALSE)\n  \n  # total number of possibilities of hands with the amount of cards specified\n  possible_hands &lt;- factorial(52)/(factorial(num_of_cards) * factorial(52-num_of_cards))\n  \n  # 13(ranks), times choosing 2 ranks out of four suits (our pair), times the \n  # number of possible cards you could choose from the remaining ranks and the \n  # amount of cards left, times the number of suits that could be pulled for the remaining cards\n  one_pair_ways &lt;- 13 * (factorial(4)/(factorial(2)*factorial(4-2))) * \n    (factorial(12)/(factorial(num_cards_left)*factorial(12 - num_cards_left))) * \n    (4 ^ num_cards_left)\n  \n  probability_of_pair &lt;- one_pair_ways/possible_hands * 100\n  \n  # when number of cards is over 5, the probability of getting only one pair \n  # becomes zero because there are only four possible suits\n  if (num_of_cards &gt; 5) {\n    print(hand)\n    return(0)\n  }\n  print(hand)\n  return(probability_of_pair)\n}\n\nprob_hands&lt;- data.frame(\n   hand_size = c(2, 3, 4, 5, 6),\n  probability = map_dbl(2:6, hand_generator)\n)\n\nJack4  Ten3 \n   50    36 \n Two2  Six1 Nine4 \n   15     6    48 \nThree3  Five2   Two4  Nine4 \n    29     18     41     48 \nFive2  Ace1 Nine3 King1  Ten2 \n   18     1    35    13    23 \n Nine4 Three4  Four3   Six4 Three3  Jack4 \n    48     42     30     45     29     50 \n\nggplot(prob_hands, aes(x = hand_size, y = probability)) +\n  geom_line() +\n  labs(title = \"Probability of Exactly One Pair in a Hand\",\n       x = \"Hand Size\",\n       y = \"Probability (%)\") +\n  theme_minimal()   \n\n\n\n\n\n\n\n\nFigure 1. Line graph of the probability of exactly one pair in a hand. The greatest probability is seen at 5 cards in a hand, and progressively decreases as hand size decreases.\nThis plot can be useful when playing card games that depend on probability. A notable example is in the card game poker, where players each have 5-card hands, and place different kinds of bets. Typically they are based on what cards they think their opponents are holding, the likelihood of certain hands being dealt, and the probabilities of drawing favorable cards to improve their own hand. A one-pair hand is seen as low value, and looking at this graph, relatively common.\nTo conquer this problem, I used an abundance of math, mostly using the factorial() function. With this, I was able to successfully calibrate the probabilities of picking up exactly one pair in hands of varying sizes. Lastly, a representation of the data was graphed using ggplot to visually represent the probabilities."
  },
  {
    "objectID": "WAI_database.html",
    "href": "WAI_database.html",
    "title": "WAI Database",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(RMariaDB)\n\nIn this analysis, I plan to create visualizations of mean absorbance by frequency in Hz of studies found in the Wideband Acoustic Intermittance (WAI) database. I based this visualization off of one created by Voss, 2019. I also plan on creating my own visualization of average absorbance by frequency by race based off of a study by Shahnaz and Bork, 2009. I plan to create a subset of the data from this source using the language SQL, and by graphing the data using ggplot in R.\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n\nSELECT \nM.Identifier, \nM.Frequency,\nCOUNT(DISTINCT M.SubjectNumber, M.Ear) AS Unique_Ears,\nAVG(M.Absorbance) AS avg_abs,\nM.Instrument, \nP.AuthorsShortList,\nP.Year,\nCONCAT(P.AuthorsShortList, \"(\", P.Year, \")\", \"N=\",COUNT(DISTINCT M.SubjectNumber, M.Ear), \";\", M.Instrument) AS label \nFROM Measurements AS M\nJOIN PI_Info AS P\nON M.Identifier = P.Identifier\nWHERE (M.Identifier = 'Abur_2014' OR M.Identifier = 'Feeney_2017' OR M.Identifier = 'Groon_2015'\nOR M.Identifier = 'Lewis_2015' OR M.Identifier = 'Liu_2008' OR M.Identifier = 'Rosowski_2012'\nOR M.Identifier = 'Shahnaz_2006' OR M.Identifier = 'Shaver_2013' OR M.Identifier = 'Sun_2016'\nOR M.Identifier = 'Voss_1994' OR M.Identifier = 'Voss_2010' OR M.Identifier = 'Werner_2010')\nAND M.Frequency &gt;= 200 AND M.Frequency &lt;= 8000\nGROUP BY M.Identifier, M.Frequency, M.Instrument, P.AuthorsShortList, P.Year;\n\n\nnew.table |&gt;\n  ggplot(aes(x = Frequency, y = avg_abs, color = label)) +\n  geom_line() +\n  labs(\n    x = \"Frequency (Hz)\",\n    y = \"Average Absorbance\",\n    title = \"Average Absorbance of Frequency by Study\"\n  )\n\n\n\n\n\n\n\n\nFigure 1. Mean absorbances by frequency for 12 studies found in the WAI database. Labels include study authors, year, number of unique ears measured, and instrument of measurement.\n\nSELECT \nM.Identifier, \nM.Frequency,\nAVG(M.Absorbance) AS avg_abs,\nS.Race\nFROM Measurements AS M\nJOIN Subjects AS S\nON M.Identifier = S.Identifier\nWHERE M.Identifier = 'Shahnaz_2006' \nAND M.Frequency &gt;= 200 AND M.Frequency &lt;= 8000\nGROUP BY M.Frequency, S.Race;\n\n\nrace.table |&gt;\n  ggplot(aes(x = Frequency, y = avg_abs, color = Race)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap( ~ Race) +\n  labs(\n    x = \"Frequency (Hz)\",\n    y = \"Average Absorbance\",\n    title = \"Average Absorbance of Frequency by Race\"\n  )\n\n\n\n\n\n\n\n\nFigure 2. Graphs by race of mean absorbance by frequency as according to Shahnaz and Bork, 2009.\n\ndbDisconnect(con_wai)\n\nIn this analysis, I successfully re-created the visualization made by Voss, 2019. I used a conbination of SQL and R in order to create a new dataset to work with, then plotted the values in that table. Then, I made my own vizualization of absorbance vs frequency based on Race from data collected by Shahnaz and Bork, 2009.\nWorks Cited\nWAI database: doi.org/10.35482/egr.001.2022\nVoss SE. Resource Review. Ear Hear. 2019 Nov/Dec;40(6):1481. doi: 10.1097/AUD.0000000000000790. PMID: 31651606; PMCID: PMC7093226.\nShahnaz, Navid1; Bork, Karin2; Polka, Linda3; Longridge, Neil4; Bell, Desmond5; Westerberg, Brian D.6. Energy Reflectance and Tympanometry in Normal and Otosclerotic Ears. Ear and Hearing 30(2):p 219-233, April 2009. | DOI: 10.1097/AUD.0b013e3181976a14"
  }
]